{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python to Integrate MongoDB Data into an ETL Process\n",
    "This notebook demonstrates the setup of an ETL (Extract, Transform, Load) pipeline.\n",
    "\n",
    "In this lab you will build upon the **Northwind_DW2** dimensional database from Lab 3; however, you will be integrating new data sourced from an instance of MongoDB. The new data will be concerned with new business processes; inventory and purchasing. You will continue to interact with both the source systems (MongoDB and MySQL), and the destination system (the Northwind_DW2 data warehouse) from a remote client running Python (Jupyter Notebooks). \n",
    "\n",
    "I fetch data into Pandas DataFrames, perform all the necessary transformations in-memory on the client, and then push the newly transformed DataFrame to the RDBMS data warehouse using a Pandas function that will create the table and fill it with data with a single operation.\n",
    "\n",
    "### Prerequisites:\n",
    "This notebook uses the PyMongo database connectivity library to connect to MySQL databases; therefore, you must have first installed that libary into your python environment by executing the following command in a Terminal window.\n",
    "\n",
    "- `python -m pip install pymongo[srv]`\n",
    "\n",
    "#### Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql\n",
    "\n",
    "\n",
    "import pymongo\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SQL Alchemy Version: 2.0.34\n",
      "Running PyMongo Version: 4.8.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running SQL Alchemy Version: {sqlalchemy.__version__}\")\n",
    "print(f\"Running PyMongo Version: {pymongo.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare & Assign Connection Variables for the MongoDB Server, the MySQL Server & Databases with which You'll be Working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "# Example setup of logging for the notebook\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to get MongoDB client\n",
    "def get_mongo_client(host: str, port: int, username: str = None, password: str = None) -> MongoClient:\n",
    "    \"\"\"Initialize MongoDB client.\"\"\"\n",
    "    if username and password:\n",
    "        client = MongoClient(host, port, username=username, password=password)\n",
    "    else:\n",
    "        client = MongoClient(host, port)\n",
    "    logger.info(\"MongoDB client initialized.\")\n",
    "    return client\n",
    "\n",
    "\n",
    "# SQL connection\n",
    "def get_sql_connection(host: str, user: str, password: str, db: str):\n",
    "    \"\"\"Initialize SQL connection.\"\"\"\n",
    "    conn = pymysql.connect(host=host, user=user, password=password, db=db)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set the path of the current working directory and append 'data' directory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData directory set to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "# Set the path of the current working directory and append 'data' directory\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "logger.info(f\"Data directory set to: {data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define JSON files for MongoDB collections\n",
    "json_files = {\n",
    "    \"sales_orders\": 'StoreSales.json',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mongo_collections(client: MongoClient, db_name: str, data_dir: str, json_files: dict):\n",
    "    \"\"\"Load JSON data into MongoDB collections.\"\"\"\n",
    "    db = client[db_name]\n",
    "    for collection_name, file_name in json_files.items():\n",
    "        file_path = os.path.abspath(os.path.join(data_dir, file_name))\n",
    "        \n",
    "        # Load JSON data and insert into MongoDB\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list): \n",
    "                    db[collection_name].insert_many(data)\n",
    "                    logger.info(f\"Inserted {len(data)} documents into '{collection_name}' collection.\")\n",
    "                else:\n",
    "                    db[collection_name].insert_one(data)\n",
    "                    logger.info(f\"Inserted a single document into '{collection_name}' collection.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data for collection '{collection_name}': {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MongoDB client initialized.\n"
     ]
    }
   ],
   "source": [
    "# MongoDB connection arguments (example)\n",
    "mongodb_args = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 27017,\n",
    "    \"username\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "    \"db_name\": \"northwind_db\"\n",
    "}\n",
    "\n",
    "# Initialize the MongoDB client\n",
    "client = get_mongo_client(\n",
    "    host=mongodb_args[\"host\"],\n",
    "    port=mongodb_args[\"port\"],\n",
    "    username=mongodb_args.get(\"username\"),\n",
    "    password=mongodb_args.get(\"password\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate MongoDB with Source Data\n",
    "You only need to run this cell once; however, the operation is *idempotent*.  In other words, it can be run multiple times without changing the end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into MongoDB collections\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], data_dir, json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Extractor\n",
    "This class provides mock methods to:\n",
    "\n",
    "Extract data from a MongoDB collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Mock MongoDB connection created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB Data:\n",
      "   _id   name  age\n",
      "0    1  Alice   30\n",
      "1    2    Bob   25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup logging for Jupyter\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Mock DatabaseConnection class (replace with actual class if available)\n",
    "class DatabaseConnection:\n",
    "    def get_mongo_connection(self):\n",
    "        # Return a mock MongoDB connection or client\n",
    "        logger.info(\"Mock MongoDB connection created.\")\n",
    "        return {\n",
    "            'my_collection': [\n",
    "                {'_id': 1, 'name': 'Alice', 'age': 30},\n",
    "                {'_id': 2, 'name': 'Bob', 'age': 25}\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def get_api_session(self):\n",
    "        import requests\n",
    "        session = requests.Session()\n",
    "        return session\n",
    "    \n",
    "    config = {\n",
    "        'api': {\n",
    "            'base_url': 'https://api.example.com'\n",
    "        }\n",
    "    }\n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, db_connection: DatabaseConnection):\n",
    "        self.db_conn = db_connection\n",
    "        \n",
    "    def extract_from_mongodb(self, collection: str, query: Dict = None) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from MongoDB collection\"\"\"\n",
    "        try:\n",
    "            mongo_db = self.db_conn.get_mongo_connection()\n",
    "            data = mongo_db.get(collection, [])\n",
    "            return pd.DataFrame(data)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from MongoDB: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def extract_from_api(self, endpoint: str, params: Dict = None) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from REST API\"\"\"\n",
    "        try:\n",
    "            session = self.db_conn.get_api_session()\n",
    "            api_config = self.db_conn.config['api']\n",
    "            response = session.get(f\"{api_config['base_url']}/{endpoint}\", params=params)\n",
    "            response.raise_for_status()\n",
    "            return pd.DataFrame(response.json())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from API: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "db_conn = DatabaseConnection()\n",
    "extractor = DataExtractor(db_conn)\n",
    "\n",
    "# Mock extraction from MongoDB\n",
    "try:\n",
    "    df_mongo = extractor.extract_from_mongodb(\"my_collection\")\n",
    "    print(\"MongoDB Data:\")\n",
    "    print(df_mongo)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in MongoDB extraction test: {str(e)}\")\n",
    "\n",
    "# Note: For `extract_from_api`, replace with an actual endpoint or mock response as required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "This class provides a mock method load_to_warehouse, simulating loading a DataFrame into a data warehouse. It logs the count of rows loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Mock SQLAlchemy engine created.\n",
      "INFO:__main__:Successfully loaded 3 rows to sample_table\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming DatabaseConnection is defined and accessible in your environment\n",
    "# Replace with the actual import if needed\n",
    "# from your_package.utils.database import DatabaseConnection\n",
    "\n",
    "\n",
    "# Mock DatabaseConnection (replace with actual implementation if available)\n",
    "class DatabaseConnection:\n",
    "    def get_sqlalchemy_engine(self):\n",
    "        # Mock SQLAlchemy engine, replace with actual database URI\n",
    "        logger.info(\"Mock SQLAlchemy engine created.\")\n",
    "        return create_engine('sqlite:///:memory:')  # Using an in-memory SQLite database for demonstration\n",
    "\n",
    "# DataLoader class\n",
    "class DataLoader:\n",
    "    def __init__(self, db_connection: DatabaseConnection):\n",
    "        self.db_conn = db_connection\n",
    "    \n",
    "    def load_to_warehouse(self, df: pd.DataFrame, table_name: str, if_exists: str = 'append') -> None:\n",
    "        \"\"\"Load DataFrame to data warehouse\"\"\"\n",
    "        try:\n",
    "            engine = self.db_conn.get_sqlalchemy_engine()\n",
    "            df.to_sql(\n",
    "                name=table_name,\n",
    "                con=engine,\n",
    "                if_exists=if_exists,\n",
    "                index=False,\n",
    "                chunksize=1000\n",
    "            )\n",
    "            logger.info(f\"Successfully loaded {len(df)} rows to {table_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data to warehouse: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Usage example\n",
    "db_connection = DatabaseConnection()\n",
    "data_loader = DataLoader(db_connection)\n",
    "\n",
    "# Create a sample DataFrame\n",
    "sample_data = {\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 30, 35]\n",
    "}\n",
    "df_sample = pd.DataFrame(sample_data)\n",
    "\n",
    "# Load the sample DataFrame into the mock database\n",
    "data_loader.load_to_warehouse(df_sample, \"sample_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populate MongoDB with Source Data\n",
    "You only need to run this cell once; however, the operation is *idempotent*.  In other words, it can be run multiple times without changing the end result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. Create and Populate the New Dimension Tables\n",
    "#### 1.1. Extract Data from the Source MongoDB Collections Into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_83801/3373813116.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['email'] = df['email'].fillna('')\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_83801/3373813116.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['phone'] = df['phone'].fillna('')\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_83801/3373813116.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['phone'] = df['phone'].apply(lambda x: ''.join(filter(str.isdigit, str(x))))\n",
      "/var/folders/hf/wdvgwf0d7_91d_lts970z1jw0000gn/T/ipykernel_83801/3373813116.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['registration_date'] = pd.to_datetime(df['registration_date'])\n",
      "INFO:__main__:Customer data cleaned successfully.\n",
      "INFO:__main__:Sales data transformed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Customer Data:\n",
      "   customer_id                email        phone registration_date\n",
      "0            1    alice@example.com  18005551212        2023-01-01\n",
      "1            2                                          2023-02-15\n",
      "2            3  charlie@example.com      5551234        2023-03-20\n",
      "\n",
      "Transformed Sales Data:\n",
      "   sale_id  quantity  unit_price  discount_rate  sale_date  total_amount  \\\n",
      "0      101         2        10.0           0.10 2023-04-01          20.0   \n",
      "1      102         5        20.0           0.20 2023-05-12         100.0   \n",
      "2      103         1        15.0           0.15 2023-06-23          15.0   \n",
      "\n",
      "   discount_amount  final_amount  sale_year  sale_month  sale_quarter  \n",
      "0             2.00         18.00       2023           4             2  \n",
      "1            20.00         80.00       2023           5             2  \n",
      "2             2.25         12.75       2023           6             2  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up logging for Jupyter\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# DataTransformer class\n",
    "class DataTransformer:\n",
    "    @staticmethod\n",
    "    def clean_customer_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and transform customer data\"\"\"\n",
    "        try:\n",
    "            # Remove duplicates\n",
    "            df = df.drop_duplicates()\n",
    "            \n",
    "            # Handle missing values\n",
    "            df['email'] = df['email'].fillna('')\n",
    "            df['phone'] = df['phone'].fillna('')\n",
    "            \n",
    "            # Standardize phone numbers\n",
    "            df['phone'] = df['phone'].apply(lambda x: ''.join(filter(str.isdigit, str(x))))\n",
    "            \n",
    "            # Convert dates to datetime\n",
    "            df['registration_date'] = pd.to_datetime(df['registration_date'])\n",
    "            \n",
    "            logger.info(\"Customer data cleaned successfully.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cleaning customer data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_sales_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform sales data\"\"\"\n",
    "        try:\n",
    "            # Calculate derived columns\n",
    "            df['total_amount'] = df['quantity'] * df['unit_price']\n",
    "            df['discount_amount'] = df['total_amount'] * df['discount_rate']\n",
    "            df['final_amount'] = df['total_amount'] - df['discount_amount']\n",
    "            \n",
    "            # Convert dates\n",
    "            df['sale_date'] = pd.to_datetime(df['sale_date'])\n",
    "            \n",
    "            # Add time dimensions\n",
    "            df['sale_year'] = df['sale_date'].dt.year\n",
    "            df['sale_month'] = df['sale_date'].dt.month\n",
    "            df['sale_quarter'] = df['sale_date'].dt.quarter\n",
    "            \n",
    "            logger.info(\"Sales data transformed successfully.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error transforming sales data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Sample usage\n",
    "# Sample customer data\n",
    "customer_data = {\n",
    "    'customer_id': [1, 2, 3, 1],\n",
    "    'email': ['alice@example.com', None, 'charlie@example.com', 'alice@example.com'],\n",
    "    'phone': ['+1-800-555-1212', None, '555-1234', '+1-800-555-1212'],\n",
    "    'registration_date': ['2023-01-01', '2023-02-15', '2023-03-20', '2023-01-01']\n",
    "}\n",
    "df_customer = pd.DataFrame(customer_data)\n",
    "\n",
    "# Sample sales data\n",
    "sales_data = {\n",
    "    'sale_id': [101, 102, 103],\n",
    "    'quantity': [2, 5, 1],\n",
    "    'unit_price': [10.0, 20.0, 15.0],\n",
    "    'discount_rate': [0.1, 0.2, 0.15],\n",
    "    'sale_date': ['2023-04-01', '2023-05-12', '2023-06-23']\n",
    "}\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "\n",
    "# Apply transformations\n",
    "df_cleaned_customer = DataTransformer.clean_customer_data(df_customer)\n",
    "df_transformed_sales = DataTransformer.transform_sales_data(df_sales)\n",
    "\n",
    "# Display results\n",
    "print(\"Cleaned Customer Data:\")\n",
    "print(df_cleaned_customer)\n",
    "\n",
    "print(\"\\nTransformed Sales Data:\")\n",
    "print(df_transformed_sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Pipeline\n",
    "This class orchestrates the ETL pipeline, coordinating the extraction, transformation, and loading stages for both customer and sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETLPipeline class\n",
    "class ETLPipeline:\n",
    "    def __init__(self):\n",
    "        self.db_conn = DatabaseConnection()\n",
    "        self.extractor = DataExtractor(self.db_conn)\n",
    "        self.transformer = DataTransformer()\n",
    "        self.loader = DataLoader(self.db_conn)\n",
    "    \n",
    "    def run_customer_pipeline(self):\n",
    "        try:\n",
    "            raw_data = self.extractor.extract_from_mongodb('customers')\n",
    "            transformed_data = self.transformer.clean_customer_data(raw_data)\n",
    "            self.loader.load_to_warehouse(transformed_data, 'dim_customer')\n",
    "            logger.info(\"Customer pipeline completed successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in customer pipeline: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_sales_pipeline(self):\n",
    "        try:\n",
    "            raw_data = self.extractor.extract_from_api('sales')\n",
    "            transformed_data = self.transformer.transform_sales_data(raw_data)\n",
    "            self.loader.load_to_warehouse(transformed_data, 'fact_sales')\n",
    "            logger.info(\"Sales pipeline completed successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in sales pipeline: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Mock DatabaseConnection initialized.\n",
      "INFO:__main__:Mock MongoDB connection.\n",
      "INFO:__main__:Mock load of 3 records to dim_customer\n",
      "INFO:__main__:Customer pipeline completed successfully\n",
      "INFO:__main__:Mock API extraction.\n",
      "INFO:__main__:Mock load of 3 records to fact_sales\n",
      "INFO:__main__:Sales pipeline completed successfully\n"
     ]
    }
   ],
   "source": [
    "etl_pipeline = ETLPipeline()\n",
    "etl_pipeline.run_customer_pipeline()\n",
    "etl_pipeline.run_sales_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_supplier_to_sql(conn, supplier_name: str, contact_name: str, country: str):\n",
    "    \"\"\"Insert a supplier into the SQL database.\"\"\"\n",
    "    query = \"\"\"\n",
    "    INSERT INTO suppliers (supplier_name, contact_name, country) \n",
    "    VALUES (%s, %s, %s);\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query, (supplier_name, contact_name, country))\n",
    "            conn.commit()\n",
    "            logger.info(f\"Supplier '{supplier_name}' inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inserting data: {str(e)}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "\n",
    "def get_suppliers_from_sql(conn):\n",
    "    \"\"\"Fetch supplier data from SQL database.\"\"\"\n",
    "    query = \"SELECT * FROM suppliers WHERE country = 'USA';\"\n",
    "    try:\n",
    "        # Use pandas to execute the query and return a DataFrame\n",
    "        return pd.read_sql(query, conn)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error executing SQL query: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Connection setup\n",
    "sql_conn = get_sql_connection(host=\"localhost\", user=\"root\", password=\"password\", db=\"retail_db\")\n",
    "\n",
    "# Fetch data from SQL\n",
    "df = get_suppliers_from_sql(sql_conn)\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retail_dataware_house_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
